{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "revised-valentine",
   "metadata": {},
   "source": [
    "# Optuna Tutorial for 10x Dataset\n",
    "This notebook shows how to use our code to train new models and do an automated hyperparameter optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "narrative-intent",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scanpy as sc\n",
    "import os\n",
    "import optuna\n",
    "import yaml\n",
    "\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "import tcr_embedding as tcr\n",
    "from tcr_embedding.constants import DONOR_SPECIFIC_ANTIGENS\n",
    "from tcr_embedding.utils_training import init_model\n",
    "from tcr_embedding.evaluation.WrapperFunctions import get_model_prediction_function\n",
    "from tcr_embedding.evaluation.Imputation import run_imputation_evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "necessary-labor",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "possible-migration",
   "metadata": {},
   "source": [
    "## USER CHOICES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "packed-veteran",
   "metadata": {},
   "outputs": [],
   "source": [
    "DONOR_NR = '1'  # options: ['1', '2']\n",
    "MODEL_TYPE = 'concat'  # options: ['RNA', 'TCR', 'concat', 'PoE']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "greenhouse-broadcasting",
   "metadata": {},
   "source": [
    "### Load Data\n",
    "First we load the data, please first download the 10x dataset and preprocess it according to the notebook in \"../preprocessing/10x_preprocessing.ipynb\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "hybrid-train",
   "metadata": {},
   "outputs": [],
   "source": [
    "adata = sc.read_h5ad('../data/10x_CD8TC/v6_supervised.h5ad')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aggregate-potential",
   "metadata": {},
   "source": [
    "This example uses cells from donor 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "imperial-moldova",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trying to set attribute `.obs` of view, copying.\n"
     ]
    }
   ],
   "source": [
    "adata = adata[adata.obs['donor'] == 'donor_'+DONOR_NR]\n",
    "adata = adata[adata.obs['binding_name'].isin(tcr.constants.DONOR_SPECIFIC_ANTIGENS[DONOR_NR])] # only use the most common and known antigen specificity\n",
    "adata.obs['binding_name'] = adata.obs['binding_name'].astype(str)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "alike-chamber",
   "metadata": {},
   "source": [
    "## Manual hyperparameter configuration\n",
    "This example shows how to use manually determined hyperparameters to initialize and train models. The hyperparameters can be specified in a config file that we load in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ranging-procurement",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'seq_model_arch': 'Transformer',\n",
       " 'seq_model_hyperparams': {'embedding_size': 32,\n",
       "  'num_heads': 4,\n",
       "  'forward_expansion': 2,\n",
       "  'encoding_layers': 2,\n",
       "  'decoding_layers': 2,\n",
       "  'dropout': 0.1},\n",
       " 'scRNA_model_arch': 'MLP',\n",
       " 'scRNA_model_hyperparams': {'gene_hidden': [1000],\n",
       "  'activation': 'leakyrelu',\n",
       "  'output_activation': 'relu',\n",
       "  'dropout': 0.2,\n",
       "  'batch_norm': True},\n",
       " 'hdim': 800,\n",
       " 'activation': 'leakyrelu',\n",
       " 'dropout': 0.2,\n",
       " 'batch_norm': True,\n",
       " 'shared_hidden': [200],\n",
       " 'zdim': 100,\n",
       " 'lr': 0.0001,\n",
       " 'batch_size': 256,\n",
       " 'losses': ['MSE', 'CE'],\n",
       " 'loss_weights': [0.1, 1.0, 5e-05]}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params = yaml.safe_load(open('../config/transformer.yaml', 'r'))\n",
    "params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "statewide-sewing",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = init_model(params, model_type=MODEL_TYPE, adata=adata, dataset_name='10x')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "satellite-subcommittee",
   "metadata": {},
   "source": [
    "**Training parameters**:\n",
    "- experiment_name (str): A name for the experiment, used to name the saved weights\n",
    "- n_epochs (int): Number of maximum epochs to train\n",
    "- batch_size (int): Batch size\n",
    "- lr (float): Learning Rate\n",
    "- loss_weights (list of len(loss_weights) == 3): weighting of losses, loss_weights[0] := RNA loss, loss_weights[1] := TCR loss, loss_weights[2] := KL loss\n",
    "- early_stop (int): Number of epochs without improvement on val loss before early stopping. Note, training won't stop before KL annealing is finished, i.e. before 30% of n_epochs. Aka patience\n",
    "- balanced_sampling (str or None): If not None: adata.obs key to select attribute to balance data on. Used to balance clonotypes, don't use for RNA-only model!\n",
    "- save_path (str): path to save the model weights\n",
    "- num_workers (int): Number of workers for PyTorch dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cubic-validation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Please increase N_EPOCHS for real training purposes. Setting it to a low value is only for demonstration purposes.\n",
    "N_EPOCHS = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "standing-offense",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create Dataloader\n",
      "Dataloader created\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  86%|████████████████████████████████▋     | 87/101 [10:28<01:41,  7.23s/it, best_epoch=11, best_f1_score=0.815]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopped\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model.train(\n",
    "    experiment_name=f'{MODEL_TYPE}_donor_{DONOR_NR}_manual',\n",
    "    n_epochs=N_EPOCHS,\n",
    "    batch_size=params['batch_size'],\n",
    "    lr=params['lr'],\n",
    "    loss_weights=params['loss_weights'],\n",
    "    early_stop=N_EPOCHS // 10,\n",
    "    balanced_sampling='clonotype',\n",
    "    save_path='../saved_models',\n",
    "    num_workers=0\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "together-armor",
   "metadata": {},
   "source": [
    "The weights with the best kNN score and best reconstruction loss are saved and can be loaded for downstream tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "atomic-gnome",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kNN weighted f1-score: 0.5849673701390998\n"
     ]
    }
   ],
   "source": [
    "file_path = os.path.join('../saved_models', f'{MODEL_TYPE}_donor_{DONOR_NR}_manual_best_knn_model.pt')\n",
    "model.load(file_path)\n",
    "test_embedding_func = get_model_prediction_function(model, batch_size=1024)  # helper function for evaluation functions\n",
    "\n",
    "summary = run_imputation_evaluation(adata, test_embedding_func, query_source='test', use_non_binder=False, use_reduced_binders=True, num_neighbors=5)\n",
    "print(f\"kNN weighted f1-score: {summary['knn']['weighted avg']['f1-score']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efficient-pencil",
   "metadata": {},
   "source": [
    "## Optuna - automated hyperparameter search\n",
    "Instead of relying on hyperparameter values from experience and gut feeling, the hyperparameters can be tuned in an automated fashion. In this tutorial we use Optuna (https://optuna.org/)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "several-arthur",
   "metadata": {},
   "source": [
    "#### Objective function for Optuna\n",
    "To perform a hyperparameter search, an objective function needs to be defined. The objective takes in the Optuna trial Object, which suggests parameters. Then the whole training and evaluation loop runs through and the objective function returns the metric back to Optuna, so Optuna knows what performance is reached with the current hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "brief-services",
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "    \"\"\"\n",
    "    Objective function for Optuna\n",
    "    :param trial: Optuna Trial Object\n",
    "    \"\"\"\n",
    "    # suggest_params is a global method, defined later on\n",
    "    params = suggest_params(trial)\n",
    "\n",
    "    # create directory to save model weights for each trial\n",
    "    save_path = f'../optuna/{name}/trial_{trial.number}'\n",
    "    if not os.path.exists(save_path):\n",
    "        os.makedirs(save_path)\n",
    "\n",
    "    model = init_model(params, model_type=MODEL_TYPE, adata=adata, dataset_name='10x')\n",
    "    \n",
    "    # Train Model\n",
    "    model.train(\n",
    "        experiment_name=name,\n",
    "        n_epochs=N_EPOCHS,\n",
    "        batch_size=params['batch_size'],\n",
    "        lr=params['lr'],\n",
    "        loss_weights=params['loss_weights'], # [] or list of floats storing weighting of loss in order [scRNA, TCR, KLD]\n",
    "        early_stop=N_EPOCHS // 10,\n",
    "        balanced_sampling='clonotype',\n",
    "        save_path=save_path,\n",
    "        num_workers=0,\n",
    "    )\n",
    "\n",
    "    # in case kNN failed from the beginning\n",
    "    if model.best_knn_metric == -1:\n",
    "        return None\n",
    "\n",
    "    return model.best_knn_metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "official-communist",
   "metadata": {},
   "outputs": [],
   "source": [
    "name = f'{MODEL_TYPE}_donor_{DONOR_NR}'\n",
    "\n",
    "import sys\n",
    "import importlib\n",
    "sys.path.append('../config_optuna')\n",
    "\n",
    "# Loads suggest_params method from the MODEL_TYPE dependent file\n",
    "suggest_params = importlib.import_module(name.lower()).suggest_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "japanese-saturn",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For demonstration purposes set to a low value\n",
    "N_TRIALS = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "banned-store",
   "metadata": {},
   "source": [
    "If the Optuna database file already exists (if training started previously), Optuna will throw an error. Delete database file manually or uncomment the corresponding lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "light-store",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-06-02 10:28:07,226]\u001b[0m A new study created in RDB with name: concat_donor_1\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create Dataloader\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:   0%|                                                                                   | 0/101 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataloader created\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 100%|██████████████████████████████████████| 101/101 [10:12<00:00,  6.06s/it, best_epoch=0, best_f1_score=0.347]\n",
      "\u001b[32m[I 2021-06-02 10:38:20,699]\u001b[0m Trial 0 finished with value: 0.3471632371949116 and parameters: {'dropout': 0.25, 'activation': 'linear', 'rna_hidden': 2000, 'hdim': 100, 'shared_hidden': 100, 'rna_num_layers': 3, 'tfmr_encoding_layers': 2, 'loss_weights_seq': 2.460285025312969, 'loss_weights_kl': 0.2657602125175934, 'batch_size': 256, 'lr': 4.753226911948138e-05, 'tfmr_embedding_size': 16, 'tfmr_num_heads': 2, 'tfmr_forward_expansion': 4, 'tfmr_dropout': 0.05, 'zdim': 50}. Best is trial 0 with value: 0.3471632371949116.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create Dataloader\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:   0%|                                                                                   | 0/101 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataloader created\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 100%|█████████████████████████████████████| 101/101 [16:45<00:00,  9.96s/it, best_epoch=57, best_f1_score=0.541]\n",
      "\u001b[32m[I 2021-06-02 10:55:07,881]\u001b[0m Trial 1 finished with value: 0.5412424072270924 and parameters: {'dropout': 0.25, 'activation': 'leakyrelu', 'rna_hidden': 2000, 'hdim': 200, 'shared_hidden': 100, 'num_layers': 2, 'rna_num_layers': 1, 'tfmr_encoding_layers': 3, 'loss_weights_seq': 0.11974500375153953, 'loss_weights_kl': 1.8403759623483707e-06, 'batch_size': 256, 'lr': 0.0005345086266269178, 'tfmr_embedding_size': 32, 'tfmr_num_heads': 4, 'tfmr_forward_expansion': 4, 'tfmr_dropout': 0.2, 'zdim': 20}. Best is trial 1 with value: 0.5412424072270924.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create Dataloader\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:   0%|                                                                                   | 0/101 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataloader created\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  42%|███████████████▊                      | 42/101 [02:55<04:06,  4.18s/it, best_epoch=21, best_f1_score=0.834]\n",
      "\u001b[32m[I 2021-06-02 10:58:04,817]\u001b[0m Trial 2 finished with value: 0.8339651561361385 and parameters: {'dropout': 0.2, 'activation': 'linear', 'rna_hidden': 1750, 'hdim': 500, 'shared_hidden': 500, 'rna_num_layers': 1, 'tfmr_encoding_layers': 2, 'loss_weights_seq': 5.48978826979483, 'loss_weights_kl': 0.0018328574596167392, 'batch_size': 512, 'lr': 0.0006459063331390998, 'tfmr_embedding_size': 16, 'tfmr_num_heads': 2, 'tfmr_forward_expansion': 2, 'tfmr_dropout': 0.0, 'zdim': 60}. Best is trial 2 with value: 0.8339651561361385.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopped\n"
     ]
    }
   ],
   "source": [
    "if not os.path.exists('../optuna/'):\n",
    "    os.makedirs('../optuna/')\n",
    "\n",
    "## Uncomment to remove previous database file to restart hyperparameter optimization, be careful! Will delete previous state\n",
    "# if os.path.exists(f'../optuna/{name}.db'):\n",
    "#     os.remove(f'../optuna/{name}.db')\n",
    "\n",
    "# Create study object\n",
    "study = optuna.create_study(study_name=name, storage=f'sqlite:///../optuna/{name}.db', direction='maximize')\n",
    "\n",
    "# Starts hyperparameter optimization\n",
    "study.optimize(objective, n_trials=N_TRIALS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "graduate-factory",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 Trials has been finished\n",
      "\n",
      "Best Trial: 2\n",
      "Best weighted f1-score on the val set: 0.8339651561361385\n",
      "Using the following parameters:\n",
      "    activation: linear\n",
      "    batch_size: 512\n",
      "    dropout: 0.2\n",
      "    hdim: 500\n",
      "    loss_weights_kl: 0.0018328574596167392\n",
      "    loss_weights_seq: 5.48978826979483\n",
      "    lr: 0.0006459063331390998\n",
      "    rna_hidden: 1750\n",
      "    rna_num_layers: 1\n",
      "    shared_hidden: 500\n",
      "    tfmr_dropout: 0.0\n",
      "    tfmr_embedding_size: 16\n",
      "    tfmr_encoding_layers: 2\n",
      "    tfmr_forward_expansion: 2\n",
      "    tfmr_num_heads: 2\n",
      "    zdim: 60\n",
      "\n",
      "You can find the saved weights here: \"../optuna/concat_donor_1/trial_2/concat_donor_1_best_knn_model.pt\"\n"
     ]
    }
   ],
   "source": [
    "print(f'{len(study.trials)} Trials has been finished\\n')\n",
    "\n",
    "best_trial = study.best_trial\n",
    "print(f'Best Trial: {best_trial.number}')\n",
    "print(f'Best weighted f1-score on the val set: {best_trial.value}')\n",
    "print(f'Using the following parameters:')\n",
    "for key, value in best_trial.params.items():\n",
    "    print(f'    {key}: {value}')\n",
    "print(f'\\nYou can find the saved weights here: \"../optuna/{name}/trial_{best_trial.number}/{name}_best_knn_model.pt\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "recognized-squad",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tcr_update] *",
   "language": "python",
   "name": "conda-env-tcr_update-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
