{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c974e860",
   "metadata": {},
   "source": [
    "# Training the model for Analysis\n",
    "In this notebook, we will explain how to train your model with the objective of dataset analysis. Here, we optimize to conserve multiple annotations at the same time (e.g. clonotype and celltype). You can determine the influence of both modalities (TCR via clonotype, GEX via celltypes) by specifying a weight for annotation. This might require retraining on a couple of weight values for finding a mixture suitable for your analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2acd6789",
   "metadata": {},
   "outputs": [],
   "source": [
    "# comet-ml must be imported before torch and sklearn\n",
    "import comet_ml\n",
    "import scanpy as sc\n",
    "\n",
    "import os\n",
    "import sys\n",
    "sys.path.append('..')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7764f2d",
   "metadata": {},
   "source": [
    "## Data Preperation\n",
    "First we load the data via the Scanpy API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "48a4abcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_data = '../data/Haniffa/haniffa_test.h5ad'\n",
    "adata = sc.read(path_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e12722b",
   "metadata": {},
   "source": [
    "Then we divide the data into training and validation data. Typically, a split of 20% validation data was used. The data is splitted to uniquely group 'clonotype' to either training or validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ee090953",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tcr_embedding.utils_training as utils\n",
    "from tcr_embedding.utils_preprocessing import group_shuffle_split\n",
    "utils.fix_seeds(42)\n",
    "\n",
    "train, val = group_shuffle_split(adata, group_col='clonotype', val_split=0.20, random_seed=42)\n",
    "adata.obs['set'] = 'train'\n",
    "adata.obs.loc[val.obs.index, 'set'] = 'val'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e384b10",
   "metadata": {},
   "source": [
    "## Defining the model parameters\n",
    "We need to proivde the model a couple of parameters:\n",
    "- study_name: Name for logging\n",
    "- comet_workspace: we logged some of the experiments via Comet-ML. This gives you more information on the training process, but is not needed. We will therefore use None here. Otherwise, specifiy the workspace name of your Comet-ML project.\n",
    "- model_name: assigns which model is used from (rna, tcr, moe, poe, concat). We will use the best performing moe.\n",
    "- balanced_sample: oversample rare elements of this column. Recommended to use a column storing the clonotype to avoid overfitting.\n",
    "- metadata: annotation to color the umaps when storing immediate results on Comet-ML. If no Comet-ML is used, pass an empty list\n",
    "- save_path: path to store the trained models over multiple training runs\n",
    "- conditional: name of a conditional variable (see preprocessing). The model partially removes batch effects over this column.\n",
    "- n_epoch: amounts of epochs to train the model. For the paper we used 200 epochs. For showcasing however, we will reduce it to 5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ee886d7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "params_experiment = {\n",
    "    'study_name': f'haniffa_tutorial',\n",
    "    'comet_workspace': None, \n",
    "    'model_name': 'moe',\n",
    "    'balanced_sampling': 'clonotype',\n",
    "    'metadata': [],\n",
    "    'save_path': '../saved_models/haniffa_tutorial',\n",
    "    'conditional': 'patient_id',\n",
    "    'n_epochs': 5,\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "920fac3b",
   "metadata": {},
   "source": [
    "## Defining Optimization parameters\n",
    "For this analysis, we will optimize to perserve clonotype and celltype (full_clustering). This optimization mode is called 'pseudo_metric'. By specifying the weight, we can choose the weighting between both modalities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1570a0a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "params_optimization = {\n",
    "    'name': 'pseudo_metric',\n",
    "    'prediction_labels':\n",
    "        {'clonotype': 1,\n",
    "         'full_clustering': 5}\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d4961ab",
   "metadata": {},
   "source": [
    "## Calling the training functions\n",
    "Finally, we need to specificy a couple of parameters for running the training. Training will be aborted either after \\<timeout\\> seconds or after having trained 3 models with 1 available GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5f207205",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-06-01 14:47:57,135]\u001b[0m A new study created in RDB with name: haniffa_tutorial\u001b[0m\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:58<00:00, 11.63s/it]\n",
      "\u001b[32m[I 2022-06-01 14:48:58,469]\u001b[0m Trial 0 finished with value: 1.4392686179739715 and parameters: {'dropout': 0.1, 'activation': 'linear', 'rna_hidden': 1500, 'hdim': 200, 'shared_hidden': 100, 'rna_num_layers': 1, 'tfmr_encoding_layers': 4, 'loss_weights_kl': 4.0428727350273357e-07, 'loss_weights_tcr': 0.034702669886504146, 'lr': 1.0994335574766187e-05, 'zdim': 50, 'tfmr_embedding_size': 16, 'tfmr_num_heads': 8, 'tfmr_dropout': 0.15000000000000002}. Best is trial 0 with value: 1.4392686179739715.\u001b[0m\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:24<00:00,  4.94s/it]\n",
      "\u001b[32m[I 2022-06-01 14:49:25,234]\u001b[0m Trial 1 finished with value: 1.4204253402130869 and parameters: {'dropout': 0.1, 'activation': 'linear', 'rna_hidden': 1000, 'hdim': 300, 'shared_hidden': 300, 'rna_num_layers': 3, 'tfmr_encoding_layers': 1, 'loss_weights_kl': 1.2173252504194046e-07, 'loss_weights_tcr': 0.009163741808778781, 'lr': 1.2385137298860926e-05, 'zdim': 35, 'tfmr_embedding_size': 64, 'tfmr_num_heads': 2, 'tfmr_dropout': 0.0}. Best is trial 0 with value: 1.4392686179739715.\u001b[0m\n",
      "  0%|                                                                                            | 0/5 [00:01<?, ?it/s]\n",
      "\u001b[33m[W 2022-06-01 14:49:28,723]\u001b[0m Trial 2 failed because of the following error: RuntimeError('CUDA out of memory. Tried to allocate 120.00 MiB (GPU 0; 4.00 GiB total capacity; 2.38 GiB already allocated; 0 bytes free; 2.53 GiB reserved in total by PyTorch)')\u001b[0m\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\felix.drost\\Anaconda3\\envs\\mvtcr_4\\lib\\site-packages\\optuna\\study\\_optimize.py\", line 213, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"..\\tcr_embedding\\models\\model_selection.py\", line 141, in <lambda>\n",
      "    study.optimize(lambda trial: objective(trial, adata, suggest_params, params_experiment, params_optimization),\n",
      "  File \"..\\tcr_embedding\\models\\model_selection.py\", line 103, in objective\n",
      "    model.train(params_experiment['n_epochs'], params_architecture['batch_size'], params_architecture['learning_rate'],\n",
      "  File \"..\\tcr_embedding\\models\\vae_base_model.py\", line 155, in train\n",
      "    train_loss_summary = self.run_epoch(epoch, phase='train')\n",
      "  File \"..\\tcr_embedding\\models\\vae_base_model.py\", line 204, in run_epoch\n",
      "    self.run_backward_pass(loss)\n",
      "  File \"..\\tcr_embedding\\models\\vae_base_model.py\", line 238, in run_backward_pass\n",
      "    loss.backward()\n",
      "  File \"C:\\Users\\felix.drost\\Anaconda3\\envs\\mvtcr_4\\lib\\site-packages\\comet_ml\\monkey_patching.py\", line 312, in wrapper\n",
      "    return_value = original(*args, **kwargs)\n",
      "  File \"C:\\Users\\felix.drost\\Anaconda3\\envs\\mvtcr_4\\lib\\site-packages\\comet_ml\\monkey_patching.py\", line 312, in wrapper\n",
      "    return_value = original(*args, **kwargs)\n",
      "  File \"C:\\Users\\felix.drost\\Anaconda3\\envs\\mvtcr_4\\lib\\site-packages\\torch\\tensor.py\", line 245, in backward\n",
      "    torch.autograd.backward(self, gradient, retain_graph, create_graph, inputs=inputs)\n",
      "  File \"C:\\Users\\felix.drost\\Anaconda3\\envs\\mvtcr_4\\lib\\site-packages\\torch\\autograd\\__init__.py\", line 145, in backward\n",
      "    Variable._execution_engine.run_backward(\n",
      "RuntimeError: CUDA out of memory. Tried to allocate 120.00 MiB (GPU 0; 4.00 GiB total capacity; 2.38 GiB already allocated; 0 bytes free; 2.53 GiB reserved in total by PyTorch)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 120.00 MiB (GPU 0; 4.00 GiB total capacity; 2.38 GiB already allocated; 0 bytes free; 2.53 GiB reserved in total by PyTorch)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32mC:\\Users\\FELIX~1.DRO\\AppData\\Local\\Temp/ipykernel_8436/1963499210.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mn_samples\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mn_gpus\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[0mrun_model_selection\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0madata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparams_experiment\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparams_optimization\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_samples\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_gpus\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\Daten\\Projects\\TcrTranscriptome\\tcr_embedding\\models\\model_selection.py\u001b[0m in \u001b[0;36mrun_model_selection\u001b[1;34m(adata, params_experiment, params_optimization, num_samples, timeout, n_jobs)\u001b[0m\n\u001b[0;32m    139\u001b[0m     \u001b[0msuggest_params\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_parameter_functions\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparams_experiment\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'model_name'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparams_optimization\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'name'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    140\u001b[0m     \u001b[1;31m# study.enqueue_trial(init_params)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 141\u001b[1;33m     study.optimize(lambda trial: objective(trial, adata, suggest_params, params_experiment, params_optimization),\n\u001b[0m\u001b[0;32m    142\u001b[0m                    n_trials=num_samples, timeout=timeout, n_jobs=n_jobs)\n\u001b[0;32m    143\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\mvtcr_4\\lib\\site-packages\\optuna\\study\\study.py\u001b[0m in \u001b[0;36moptimize\u001b[1;34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[0;32m    398\u001b[0m             )\n\u001b[0;32m    399\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 400\u001b[1;33m         _optimize(\n\u001b[0m\u001b[0;32m    401\u001b[0m             \u001b[0mstudy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    402\u001b[0m             \u001b[0mfunc\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\mvtcr_4\\lib\\site-packages\\optuna\\study\\_optimize.py\u001b[0m in \u001b[0;36m_optimize\u001b[1;34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[0;32m     64\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mn_jobs\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 66\u001b[1;33m             _optimize_sequential(\n\u001b[0m\u001b[0;32m     67\u001b[0m                 \u001b[0mstudy\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     68\u001b[0m                 \u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\mvtcr_4\\lib\\site-packages\\optuna\\study\\_optimize.py\u001b[0m in \u001b[0;36m_optimize_sequential\u001b[1;34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001b[0m\n\u001b[0;32m    161\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    162\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 163\u001b[1;33m             \u001b[0mtrial\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_run_trial\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstudy\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcatch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    164\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    165\u001b[0m             \u001b[1;32mraise\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\mvtcr_4\\lib\\site-packages\\optuna\\study\\_optimize.py\u001b[0m in \u001b[0;36m_run_trial\u001b[1;34m(study, func, catch)\u001b[0m\n\u001b[0;32m    262\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    263\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mstate\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mTrialState\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mFAIL\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mfunc_err\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfunc_err\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcatch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 264\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0mfunc_err\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    265\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mtrial\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    266\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\mvtcr_4\\lib\\site-packages\\optuna\\study\\_optimize.py\u001b[0m in \u001b[0;36m_run_trial\u001b[1;34m(study, func, catch)\u001b[0m\n\u001b[0;32m    211\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    212\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 213\u001b[1;33m         \u001b[0mvalue_or_values\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    214\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mexceptions\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTrialPruned\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    215\u001b[0m         \u001b[1;31m# TODO(mamu): Handle multi-objective cases.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Daten\\Projects\\TcrTranscriptome\\tcr_embedding\\models\\model_selection.py\u001b[0m in \u001b[0;36m<lambda>\u001b[1;34m(trial)\u001b[0m\n\u001b[0;32m    139\u001b[0m     \u001b[0msuggest_params\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_parameter_functions\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparams_experiment\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'model_name'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparams_optimization\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'name'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    140\u001b[0m     \u001b[1;31m# study.enqueue_trial(init_params)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 141\u001b[1;33m     study.optimize(lambda trial: objective(trial, adata, suggest_params, params_experiment, params_optimization),\n\u001b[0m\u001b[0;32m    142\u001b[0m                    n_trials=num_samples, timeout=timeout, n_jobs=n_jobs)\n\u001b[0;32m    143\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Daten\\Projects\\TcrTranscriptome\\tcr_embedding\\models\\model_selection.py\u001b[0m in \u001b[0;36mobjective\u001b[1;34m(trial, adata_tmp, suggest_params, params_experiment_base, optimization_mode_params)\u001b[0m\n\u001b[0;32m    101\u001b[0m                   params_experiment['label_key'], params_experiment['device'])\n\u001b[0;32m    102\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 103\u001b[1;33m     model.train(params_experiment['n_epochs'], params_architecture['batch_size'], params_architecture['learning_rate'],\n\u001b[0m\u001b[0;32m    104\u001b[0m                 \u001b[0mparams_architecture\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'loss_weights'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparams_experiment\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'kl_annealing_epochs'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    105\u001b[0m                 params_experiment['early_stop'], params_experiment['save_path'], comet)\n",
      "\u001b[1;32mC:\\Daten\\Projects\\TcrTranscriptome\\tcr_embedding\\models\\vae_base_model.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self, n_epochs, batch_size, learning_rate, loss_weights, kl_annealing_epochs, early_stop, save_path, comet)\u001b[0m\n\u001b[0;32m    153\u001b[0m                 \u001b[1;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_epochs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    154\u001b[0m                         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 155\u001b[1;33m                         \u001b[0mtrain_loss_summary\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_epoch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mphase\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'train'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    156\u001b[0m                         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlog_losses\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_loss_summary\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    157\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Daten\\Projects\\TcrTranscriptome\\tcr_embedding\\models\\vae_base_model.py\u001b[0m in \u001b[0;36mrun_epoch\u001b[1;34m(self, epoch, phase)\u001b[0m\n\u001b[0;32m    202\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    203\u001b[0m                         \u001b[1;32mif\u001b[0m \u001b[0mphase\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'train'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 204\u001b[1;33m                                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_backward_pass\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    205\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    206\u001b[0m                         \u001b[0mloss_total\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Daten\\Projects\\TcrTranscriptome\\tcr_embedding\\models\\vae_base_model.py\u001b[0m in \u001b[0;36mrun_backward_pass\u001b[1;34m(self, loss)\u001b[0m\n\u001b[0;32m    236\u001b[0m         \u001b[1;32mdef\u001b[0m \u001b[0mrun_backward_pass\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    237\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 238\u001b[1;33m                 \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    239\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptimization_mode_params\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;34m'grad_clip'\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptimization_mode_params\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    240\u001b[0m                         \u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclip_grad_value_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptimization_mode_params\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'grad_clip'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\mvtcr_4\\lib\\site-packages\\comet_ml\\monkey_patching.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    310\u001b[0m                     )\n\u001b[0;32m    311\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 312\u001b[1;33m         \u001b[0mreturn_value\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0moriginal\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    313\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    314\u001b[0m         \u001b[1;31m# Call after callbacks once we have the return value\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\mvtcr_4\\lib\\site-packages\\comet_ml\\monkey_patching.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    310\u001b[0m                     )\n\u001b[0;32m    311\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 312\u001b[1;33m         \u001b[0mreturn_value\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0moriginal\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    313\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    314\u001b[0m         \u001b[1;31m# Call after callbacks once we have the return value\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\mvtcr_4\\lib\\site-packages\\torch\\tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    243\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    244\u001b[0m                 inputs=inputs)\n\u001b[1;32m--> 245\u001b[1;33m         \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    246\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    247\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\mvtcr_4\\lib\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    143\u001b[0m         \u001b[0mretain_graph\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    144\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 145\u001b[1;33m     Variable._execution_engine.run_backward(\n\u001b[0m\u001b[0;32m    146\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    147\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n",
      "\u001b[1;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 120.00 MiB (GPU 0; 4.00 GiB total capacity; 2.38 GiB already allocated; 0 bytes free; 2.53 GiB reserved in total by PyTorch)"
     ]
    }
   ],
   "source": [
    "from tcr_embedding.models.model_selection import run_model_selection\n",
    "\n",
    "timeout = (20*60)\n",
    "n_samples = 3\n",
    "n_gpus = 1\n",
    "run_model_selection(adata, params_experiment, params_optimization, n_samples, timeout, n_gpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c4d61e8",
   "metadata": {},
   "source": [
    "## Output\n",
    "The console output indicates the best model after Hyperparameter Optimization. We will now load this model and embedd our data with it. Following, we can continue with standard analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0354d1db",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_model = '../saved_models/haniffa_tutorial/trial_0/best_model_by_metric.pt'\n",
    "model = utils.load_model(adata, path_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be35dca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_moe = moe_model.get_latent(data, metadata=[], return_mean=True)\n",
    "latent_moe.obs = adata.obs.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf0500d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.pp.neighbors(latent_moe, use_rep='X')\n",
    "sc.tl.umap(latent_moe)\n",
    "sc.pl.umap(latent_moe, color='full_clustering')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed5f1645",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:mvtcr_4]",
   "language": "python",
   "name": "conda-env-mvtcr_4-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
