seq_model_arch: BiGRU
seq_model_hyperparams:
  embedding_dim: 16
  hidden_size: 256
  num_layers: 2
  dropout: 0.2
  bidirectional: True

scRNA_model_arch: MLP
scRNA_model_hyperparams:
  gene_hidden:
    - 1024
    - 512
  activation: relu
  output_activation: linear
  dropout: 0.2
  batch_norm: True

# shared param by all models, "interface" between the models
hdim: 128

# params of shared encoder and decoder
activation: relu
dropout: 0.2
batch_norm: True
shared_hidden:
  - 64
zdim: 16
